"""Event publisher for Dapr Pub/Sub.

Implements: 009-dapr-pubsub-events spec - Event Publishing
Uses fire-and-forget pattern - task operations succeed even if publishing fails.
"""
import logging
import os
import uuid
from datetime import datetime
from typing import Any, Dict, Optional

import httpx

logger = logging.getLogger(__name__)

# Configuration from environment
DAPR_HTTP_PORT = os.getenv("DAPR_HTTP_PORT", "3500")
PUBSUB_NAME = os.getenv("DAPR_PUBSUB_NAME", "kafka-pubsub")


class EventPublisher:
    """Dapr Pub/Sub event publisher.

    Publishes events in CloudEvents 1.0 format via Dapr HTTP API.
    Implements fire-and-forget pattern - failures are logged but don't
    block the calling operation.
    """

    def __init__(
        self,
        dapr_port: Optional[str] = None,
        pubsub_name: Optional[str] = None
    ):
        """Initialize event publisher.

        Args:
            dapr_port: Dapr HTTP port (default from DAPR_HTTP_PORT env)
            pubsub_name: Pub/Sub component name (default from DAPR_PUBSUB_NAME env)
        """
        self.dapr_port = dapr_port or DAPR_HTTP_PORT
        self.pubsub_name = pubsub_name or PUBSUB_NAME
        self.base_url = f"http://localhost:{self.dapr_port}/v1.0/publish/{self.pubsub_name}"
        self._client: Optional[httpx.AsyncClient] = None

    async def _get_client(self) -> httpx.AsyncClient:
        """Get or create async HTTP client."""
        if self._client is None or self._client.is_closed:
            self._client = httpx.AsyncClient(timeout=5.0)
        return self._client

    async def close(self):
        """Close the HTTP client."""
        if self._client and not self._client.is_closed:
            await self._client.aclose()

    async def publish(
        self,
        topic: str,
        event_type: str,
        data: Dict[str, Any],
        user_id: str,
        source: str = "todo-backend"
    ) -> bool:
        """Publish event to Dapr Pub/Sub topic.

        Uses CloudEvents 1.0 format with user_id as partition key
        for event ordering per user.

        Args:
            topic: Target topic name (e.g., "tasks", "reminders")
            event_type: Event type (e.g., "task.created", "task.completed")
            data: Event payload data
            user_id: User ID for partition key (ensures per-user ordering)
            source: Event source (default "todo-backend", use "handler" for
                    events generated by handlers to prevent circular triggers)

        Returns:
            True if publish successful, False otherwise
        """
        event_id = str(uuid.uuid4())
        now = datetime.utcnow()

        # Build CloudEvent
        cloud_event = {
            "specversion": "1.0",
            "type": event_type,
            "source": source,
            "id": event_id,
            "time": now.isoformat() + "Z",
            "partitionkey": user_id,
            "data": {
                "user_id": user_id,
                **data,
                "timestamp": now.isoformat() + "Z"
            }
        }

        try:
            client = await self._get_client()
            url = f"{self.base_url}/{topic}"

            response = await client.post(
                url,
                json=cloud_event,
                headers={"Content-Type": "application/cloudevents+json"}
            )

            if response.status_code in (200, 201, 204):
                logger.info(f"Published {event_type} event {event_id} to {topic}")
                return True
            else:
                logger.warning(
                    f"Failed to publish {event_type}: "
                    f"{response.status_code} - {response.text}"
                )
                return False

        except httpx.ConnectError as e:
            # Fire-and-forget: log but don't fail the operation
            logger.warning(f"Dapr Pub/Sub unavailable, event not published: {e}")
            return False
        except Exception as e:
            logger.error(f"Error publishing {event_type} event: {e}")
            return False


# Singleton instance
_publisher_instance: Optional[EventPublisher] = None


def get_publisher() -> EventPublisher:
    """Get or create singleton EventPublisher instance."""
    global _publisher_instance
    if _publisher_instance is None:
        _publisher_instance = EventPublisher()
    return _publisher_instance


async def publish_event(
    topic: str,
    event_type: str,
    data: Dict[str, Any],
    user_id: str,
    source: str = "todo-backend"
) -> bool:
    """Convenience function to publish an event.

    Fire-and-forget pattern - failures are logged but don't raise exceptions.

    Args:
        topic: Target topic name
        event_type: Event type
        data: Event payload
        user_id: User ID for partition key
        source: Event source (use "handler" for handler-generated events)

    Returns:
        True if published successfully
    """
    publisher = get_publisher()
    return await publisher.publish(topic, event_type, data, user_id, source)
