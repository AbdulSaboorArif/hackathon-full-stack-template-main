"""Event subscription handlers for Dapr Pub/Sub.

Implements: 009-dapr-pubsub-events spec - Subscription Handlers
Handles task events for recurring task generation and reminder scheduling.

Error Handling:
- Returns 200 OK for successful processing (Dapr ACK)
- Returns 500 for transient failures (Dapr retry)
- Malformed data is logged and ACKed (no retry for bad data)
- Circular triggers are prevented via source tracking
- DLQ events generate alerts (alert-only, no auto-retry)
- Circuit breaker prevents cascading failures from external services
"""
import logging
from datetime import datetime, timedelta
from typing import Dict, Any, Optional, Set, Callable
from functools import wraps
import asyncio

from dateutil.relativedelta import relativedelta
from sqlmodel.ext.asyncio.session import AsyncSession

from app.events.publisher import publish_event

logger = logging.getLogger(__name__)

# Track processed events for idempotency (in-memory for now)
# In production, use Redis or database table
_processed_events: Set[str] = set()
MAX_PROCESSED_EVENTS = 10000  # Limit memory usage

# Track event sources to prevent circular triggers
# Events generated by handlers use "handler" source
HANDLER_SOURCE = "handler"
BACKEND_SOURCE = "todo-backend"


# ============================================================================
# Circuit Breaker Implementation (009-047)
# ============================================================================

class CircuitBreaker:
    """Simple circuit breaker for external service calls.

    States:
    - CLOSED: Normal operation, requests pass through
    - OPEN: Requests fail immediately (after threshold failures)
    - HALF_OPEN: Test if service recovered (after reset_timeout)

    Implements: 009-047 Add circuit breaker for external service calls
    """

    CLOSED = "closed"
    OPEN = "open"
    HALF_OPEN = "half_open"

    def __init__(
        self,
        failure_threshold: int = 5,
        reset_timeout: float = 30.0,
        name: str = "default"
    ):
        """Initialize circuit breaker.

        Args:
            failure_threshold: Number of failures before opening circuit
            reset_timeout: Seconds to wait before trying again (half-open)
            name: Name for logging
        """
        self.failure_threshold = failure_threshold
        self.reset_timeout = reset_timeout
        self.name = name
        self.state = self.CLOSED
        self.failure_count = 0
        self.last_failure_time: Optional[datetime] = None

    def _should_allow_request(self) -> bool:
        """Check if request should be allowed based on circuit state."""
        if self.state == self.CLOSED:
            return True

        if self.state == self.OPEN:
            # Check if reset timeout has passed
            if self.last_failure_time:
                elapsed = (datetime.utcnow() - self.last_failure_time).total_seconds()
                if elapsed >= self.reset_timeout:
                    self.state = self.HALF_OPEN
                    logger.info(f"Circuit breaker '{self.name}' entering HALF_OPEN state")
                    return True
            return False

        # HALF_OPEN: allow one request to test
        return True

    def record_success(self) -> None:
        """Record successful call."""
        if self.state == self.HALF_OPEN:
            self.state = self.CLOSED
            self.failure_count = 0
            logger.info(f"Circuit breaker '{self.name}' CLOSED (recovered)")
        elif self.state == self.CLOSED:
            self.failure_count = 0

    def record_failure(self) -> None:
        """Record failed call."""
        self.failure_count += 1
        self.last_failure_time = datetime.utcnow()

        if self.state == self.HALF_OPEN:
            self.state = self.OPEN
            logger.warning(f"Circuit breaker '{self.name}' OPEN (half-open test failed)")
        elif self.failure_count >= self.failure_threshold:
            self.state = self.OPEN
            logger.warning(
                f"Circuit breaker '{self.name}' OPEN "
                f"(threshold {self.failure_threshold} reached)"
            )

    def is_open(self) -> bool:
        """Check if circuit is open (blocking requests)."""
        return not self._should_allow_request()


# Global circuit breakers for different services
_circuit_breakers: Dict[str, CircuitBreaker] = {}


def get_circuit_breaker(name: str = "default") -> CircuitBreaker:
    """Get or create a circuit breaker by name."""
    if name not in _circuit_breakers:
        _circuit_breakers[name] = CircuitBreaker(name=name)
    return _circuit_breakers[name]


def with_circuit_breaker(breaker_name: str = "default"):
    """Decorator to wrap async functions with circuit breaker protection.

    Usage:
        @with_circuit_breaker("database")
        async def query_database():
            ...
    """
    def decorator(func: Callable):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            breaker = get_circuit_breaker(breaker_name)

            if breaker.is_open():
                logger.warning(
                    f"Circuit breaker '{breaker_name}' is OPEN, "
                    f"skipping call to {func.__name__}"
                )
                return None

            try:
                result = await func(*args, **kwargs)
                breaker.record_success()
                return result
            except Exception as e:
                breaker.record_failure()
                logger.error(f"Circuit breaker '{breaker_name}' recorded failure: {e}")
                raise

        return wrapper
    return decorator


def _is_event_processed(event_id: str) -> bool:
    """Check if event was already processed (idempotency)."""
    return event_id in _processed_events


def _mark_event_processed(event_id: str) -> None:
    """Mark event as processed."""
    global _processed_events
    _processed_events.add(event_id)
    # Cleanup if too many events (simple LRU-like behavior)
    if len(_processed_events) > MAX_PROCESSED_EVENTS:
        # Remove oldest half
        _processed_events = set(list(_processed_events)[MAX_PROCESSED_EVENTS // 2:])


def _is_handler_generated(event_source: str) -> bool:
    """Check if event was generated by a handler (circular trigger prevention).

    Args:
        event_source: The 'source' field from CloudEvent

    Returns:
        True if event was generated by handler, should not trigger handler again
    """
    return event_source == HANDLER_SOURCE


def _validate_event_data(event_data: Dict[str, Any], required_fields: list) -> bool:
    """Validate that event data contains required fields.

    Args:
        event_data: Event payload to validate
        required_fields: List of field names that must be present

    Returns:
        True if all required fields present, False otherwise
    """
    for field in required_fields:
        if field not in event_data:
            logger.warning(f"Missing required field '{field}' in event data")
            return False
    return True


def log_dlq_alert(event_id: str, event_type: str, error_message: str) -> None:
    """Log alert for events reaching Dead Letter Queue.

    Implements: 009-048 DLQ alert logging (alert-only, no auto-retry)

    Args:
        event_id: CloudEvent ID
        event_type: Event type (e.g., task.created)
        error_message: Error that caused DLQ
    """
    logger.error(
        f"[DLQ ALERT] Event reached dead letter queue - "
        f"Event ID: {event_id}, Type: {event_type}, Error: {error_message}. "
        f"Manual review required."
    )


def calculate_next_due_date(current_due: datetime, interval: str) -> datetime:
    """Calculate next due date based on recurring interval.

    Args:
        current_due: Current due date
        interval: "daily", "weekly", or "monthly"

    Returns:
        Next due date
    """
    if interval == "daily":
        return current_due + timedelta(days=1)
    elif interval == "weekly":
        return current_due + timedelta(weeks=1)
    elif interval == "monthly":
        return current_due + relativedelta(months=1)
    else:
        # Default to daily if unknown interval
        logger.warning(f"Unknown interval '{interval}', defaulting to daily")
        return current_due + timedelta(days=1)


async def handle_task_created(
    event_data: Dict[str, Any],
    event_id: str,
    session: Optional[AsyncSession] = None
) -> Dict[str, Any]:
    """Handle task.created event - schedule reminder if due_date set.

    Args:
        event_data: Event payload
        event_id: CloudEvent ID for idempotency
        session: Optional database session

    Returns:
        Handler result dict
    """
    # Idempotency check
    if _is_event_processed(event_id):
        logger.info(f"Event {event_id} already processed, skipping")
        return {"status": "skipped", "reason": "duplicate"}

    task_id = event_data.get("task_id")
    user_id = event_data.get("user_id")
    due_date_str = event_data.get("due_date")

    logger.info(f"Processing task.created for task {task_id}")

    result = {"status": "ok", "actions": []}

    # Schedule reminder if due_date is set and in the future
    if due_date_str:
        try:
            due_date = datetime.fromisoformat(due_date_str.replace("Z", "+00:00"))
            remind_at = due_date - timedelta(hours=24)

            if remind_at > datetime.now(due_date.tzinfo):
                # Publish reminder.scheduled event
                # Use HANDLER_SOURCE since this is triggered by a handler
                await publish_event(
                    topic="reminders",
                    event_type="reminder.scheduled",
                    data={
                        "task_id": task_id,
                        "remind_at": remind_at.isoformat()
                    },
                    user_id=user_id,
                    source=HANDLER_SOURCE
                )
                result["actions"].append("reminder_scheduled")
                logger.info(f"Scheduled reminder for task {task_id} at {remind_at}")
        except Exception as e:
            logger.warning(f"Failed to schedule reminder for task {task_id}: {e}")

    _mark_event_processed(event_id)
    return result


async def handle_task_updated(
    event_data: Dict[str, Any],
    event_id: str,
    session: Optional[AsyncSession] = None
) -> Dict[str, Any]:
    """Handle task.updated event - reschedule reminder if due_date changed.

    Args:
        event_data: Event payload
        event_id: CloudEvent ID for idempotency
        session: Optional database session

    Returns:
        Handler result dict
    """
    if _is_event_processed(event_id):
        logger.info(f"Event {event_id} already processed, skipping")
        return {"status": "skipped", "reason": "duplicate"}

    task_id = event_data.get("task_id")
    user_id = event_data.get("user_id")
    changed_fields = event_data.get("changed_fields", [])

    logger.info(f"Processing task.updated for task {task_id}, changed: {changed_fields}")

    result = {"status": "ok", "actions": []}

    # If due_date changed, reschedule reminder
    if "due_date" in changed_fields:
        due_date_str = event_data.get("due_date")
        if due_date_str:
            try:
                due_date = datetime.fromisoformat(due_date_str.replace("Z", "+00:00"))
                remind_at = due_date - timedelta(hours=24)

                if remind_at > datetime.now(due_date.tzinfo):
                    # Use HANDLER_SOURCE since this is triggered by a handler
                    await publish_event(
                        topic="reminders",
                        event_type="reminder.scheduled",
                        data={
                            "task_id": task_id,
                            "remind_at": remind_at.isoformat()
                        },
                        user_id=user_id,
                        source=HANDLER_SOURCE
                    )
                    result["actions"].append("reminder_rescheduled")
            except Exception as e:
                logger.warning(f"Failed to reschedule reminder: {e}")

    _mark_event_processed(event_id)
    return result


async def handle_task_completed(
    event_data: Dict[str, Any],
    event_id: str,
    session: Optional[AsyncSession] = None
) -> Dict[str, Any]:
    """Handle task.completed event - create next recurring instance.

    Args:
        event_data: Event payload
        event_id: CloudEvent ID for idempotency
        session: Database session for creating new task

    Returns:
        Handler result dict
    """
    if _is_event_processed(event_id):
        logger.info(f"Event {event_id} already processed, skipping")
        return {"status": "skipped", "reason": "duplicate"}

    task_id = event_data.get("task_id")
    user_id = event_data.get("user_id")
    completed = event_data.get("completed", False)
    is_recurring = event_data.get("is_recurring", False)
    interval = event_data.get("recurring_interval")
    due_date_str = event_data.get("due_date")

    logger.info(f"Processing task.completed for task {task_id}, completed={completed}")

    result = {"status": "ok", "actions": []}

    # Only process if task was marked complete (not uncomplete)
    if not completed:
        _mark_event_processed(event_id)
        return result

    # Create next instance for recurring tasks
    if is_recurring and interval and due_date_str and session:
        try:
            due_date = datetime.fromisoformat(due_date_str.replace("Z", "+00:00"))
            next_due = calculate_next_due_date(due_date, interval)

            # Import here to avoid circular dependency
            from app.models import Task

            # Get original task data to copy
            from sqlmodel import select
            stmt = select(Task).where(Task.id == task_id, Task.user_id == user_id)
            db_result = await session.execute(stmt)
            original_task = db_result.scalar_one_or_none()

            if original_task:
                # Create new task instance
                new_task = Task(
                    user_id=user_id,
                    title=original_task.title,
                    description=original_task.description,
                    priority=original_task.priority,
                    tags=original_task.tags,
                    due_date=next_due,
                    is_recurring=True,
                    recurring_interval=interval,
                    completed=False,
                    created_at=datetime.utcnow(),
                    updated_at=datetime.utcnow()
                )
                session.add(new_task)
                await session.flush()

                result["actions"].append("recurring_instance_created")
                result["new_task_id"] = new_task.id
                logger.info(f"Created next recurring instance: {new_task.id}")

                # Publish task.created for the new instance
                # Use HANDLER_SOURCE to prevent circular triggers
                await publish_event(
                    topic="tasks",
                    event_type="task.created",
                    data={
                        "task_id": new_task.id,
                        "title": new_task.title,
                        "description": new_task.description,
                        "priority": new_task.priority,
                        "tags": new_task.tags,
                        "due_date": next_due.isoformat() if next_due else None,
                        "is_recurring": True,
                        "recurring_interval": interval
                    },
                    user_id=user_id,
                    source=HANDLER_SOURCE
                )

        except Exception as e:
            logger.error(f"Failed to create recurring instance: {e}")
            result["error"] = str(e)

    _mark_event_processed(event_id)
    return result


async def handle_task_deleted(
    event_data: Dict[str, Any],
    event_id: str,
    session: Optional[AsyncSession] = None
) -> Dict[str, Any]:
    """Handle task.deleted event - cancel pending reminders.

    Args:
        event_data: Event payload
        event_id: CloudEvent ID for idempotency
        session: Optional database session

    Returns:
        Handler result dict
    """
    if _is_event_processed(event_id):
        logger.info(f"Event {event_id} already processed, skipping")
        return {"status": "skipped", "reason": "duplicate"}

    task_id = event_data.get("task_id")
    user_id = event_data.get("user_id")

    logger.info(f"Processing task.deleted for task {task_id}")

    result = {"status": "ok", "actions": []}

    # Cancel any scheduled reminders via Dapr Jobs API
    # Note: This would use Dapr Jobs API in production
    # For now, just log the cancellation intent
    logger.info(f"Would cancel reminders for task {task_id}")
    result["actions"].append("reminders_cancelled")

    _mark_event_processed(event_id)
    return result


async def handle_reminder_scheduled(
    event_data: Dict[str, Any],
    event_id: str,
    session: Optional[AsyncSession] = None
) -> Dict[str, Any]:
    """Handle reminder.scheduled event - schedule Dapr job.

    Args:
        event_data: Event payload
        event_id: CloudEvent ID for idempotency
        session: Optional database session

    Returns:
        Handler result dict
    """
    if _is_event_processed(event_id):
        return {"status": "skipped", "reason": "duplicate"}

    task_id = event_data.get("task_id")
    remind_at = event_data.get("remind_at")

    logger.info(f"Processing reminder.scheduled for task {task_id} at {remind_at}")

    # In production, this would create a Dapr scheduled job
    # For now, just acknowledge the event
    result = {"status": "ok", "actions": ["job_scheduled"]}

    _mark_event_processed(event_id)
    return result


async def handle_reminder_triggered(
    event_data: Dict[str, Any],
    event_id: str,
    session: Optional[AsyncSession] = None
) -> Dict[str, Any]:
    """Handle reminder.triggered event - create in-app notification.

    Args:
        event_data: Event payload
        event_id: CloudEvent ID for idempotency
        session: Database session for creating notification

    Returns:
        Handler result dict
    """
    if _is_event_processed(event_id):
        return {"status": "skipped", "reason": "duplicate"}

    task_id = event_data.get("task_id")
    user_id = event_data.get("user_id")
    task_title = event_data.get("task_title")
    due_date = event_data.get("due_date")

    logger.info(f"Processing reminder.triggered for task {task_id}")

    result = {"status": "ok", "actions": []}

    # Create in-app notification
    # Note: Notification model/table would be created in a future task
    logger.info(
        f"Would create notification for user {user_id}: "
        f"Task '{task_title}' due soon at {due_date}"
    )
    result["actions"].append("notification_created")

    _mark_event_processed(event_id)
    return result
